{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2ac909",
   "metadata": {},
   "source": [
    "Problem Statement: \n",
    "You have been provided with a multi dimensional data that contains information on certain images. Using machine learning, you should be able to predict the images on the new set of data using the model that you have trained on the existing data. \n",
    "\n",
    "Dataset Information: Each point in the data is an 8Ã—8 image.\n",
    "\n",
    "Classes\t10\n",
    "\n",
    "Samples per class\t~180\n",
    "\n",
    "Samples total\t1797\n",
    "\n",
    "Dimensionality\t64\n",
    "\n",
    "Features\tintegers 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2d31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA is an unsupervised technique for dimensionality reduction\n"
     ]
    }
   ],
   "source": [
    "# 1) PCA is ________?\n",
    "# Unsupervised      ans\n",
    "# Semisupervised\n",
    "# None of the above\n",
    "# Supervised\n",
    "\n",
    "print('PCA is an unsupervised technique for dimensionality reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Why is PCA needed in the Data Science field?\n",
    "# Data Visualization\n",
    "# All of the Above\n",
    "# Data Manipulation\n",
    "# In Dimensionality Reduction     ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151de335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Extract only features and scale the data using StandardScaler() , compute min covariance?\n",
    "# 0.1\n",
    "# -1\n",
    "# -0.57\n",
    "# None of the above  ans\n",
    "# Covariance measures the linear relationship between two features. Minimum covariance doesn't hold much significance in PCA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a75a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Extract only features and scale the data using StandardScaler() , how many eigen_values will be calculated?\n",
    "# 1\n",
    "# 64 ans\n",
    "# 1797\n",
    "# 0\n",
    "\n",
    "#Number of eigenvalues will be equal to the number of features after scaling  8*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9103fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) from the above eigenvalues and eigenvectors, create eigenpair and calculate maximum cumulative explained variance?\n",
    "# 80\n",
    "# 69\n",
    "# 90  \n",
    "# 100\n",
    "\n",
    "# You'll need to decide on a threshold (e.g., 80%, 90%) for the desired level of information retention.\n",
    "#Then, sum the eigenvalues of the top PCs that meet or exceed that threshold to determine \n",
    "#the maximum cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e13e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Why is cumulative explained variance required before applying PCA?\n",
    "# To check the maximum components to be selected  ans\n",
    "# All of the above\n",
    "# To check the distribution of the data\n",
    "# To check the minimum components to be selected\n",
    "\n",
    "# Cumulative explained variance helps in determining the number of principal components to retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce22657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Fit the data to PCA and compute the maximum explained_variance_ratio?\n",
    "\n",
    "# 0.12\n",
    "# 0.012\n",
    "# 0.011\n",
    "# 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9] Which of the following is the correct syntax for finding the eigenvalues and\n",
    "# eigenvectors with give covariance matric (cov_mat)?\n",
    "# eig_vals, eig_vecs =np.linalg.eig(cov_mat)  ans\n",
    "# eig_vals, eig_vecs =np.linalg.eig()\n",
    "# eig_vals, eig_vecs =np.linalg(cov_mat)\n",
    "# eig_vals, eig_vecs =np.eigen_val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f63c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Choose the correct order of steps to choose principal components:\n",
    "# A . compute the covariance matrix. B. choose the principal components from Eigen values and Vectors. \n",
    "# C. collecting the data. D. compute Eigen values and vectors. E. standardization.\n",
    "\n",
    "# A->C->E->D->B\n",
    "# C->E->A->D->B     ans\n",
    "# none of the above\n",
    "# E->D->C->D->B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0c4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) What correction is required in the following lines of code to create covariance matrix?\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# standardized_data = scaler.fit_transform(data)\n",
    "# covariance_matrix = np.cov(standardized_data)\n",
    "\n",
    "# All of the above.\n",
    "# Need to find eigen values and eigen Vectors before standardization.\n",
    "# Need to perform transpose of standardized_data  ans\n",
    "# Need to flatten the standardized_data.\n",
    "\n",
    "\n",
    "# When computing the covariance matrix, it should be based on features as columns, \n",
    "# so transposing the standardized data is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edaa66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12)What exactly do you understand by the given code?\n",
    "\n",
    "# pca = PCA(n_components=0.95)\n",
    "\n",
    "# We are looking for the principal components that cumulatively explain 95% of the variance in the data. ans\n",
    "# We are looking for the components that accumulated to the total of 95% of the mean in the data.\n",
    "# We are looking for the components that explains 95% of the standard deviation in the data.\n",
    "# The components should have 0.95 probability in the dataset.\n",
    "\n",
    "# The n_components parameter in PCA specifies the number of components to keep, \n",
    "# and setting it to a value between 0 and 1 indicates the proportion of variance to be explained by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c368422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) How is the first principal component determined in PCA?\n",
    "\n",
    "# It is the component with the least variance.\n",
    "# It is the component with the median variance.\n",
    "# It is randomly selected.\n",
    "# It is the component with the most variance. ans\n",
    "\n",
    "# PCA identifies the directions (principal components) that maximize the variance in the data, \n",
    "# so the first principal component captures the most variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2f920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Which of the following non-zero vector stays parallel after matrix multiplication\n",
    "\n",
    "# Covariance vector\n",
    "# EigenValue\n",
    "# EigenVector ans\n",
    "# Explained variance ratio\n",
    "\n",
    "#  Eigenvectors are the vectors that remain in the same direction (parallel) after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a429e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) The Output of PCA is always a new representation of data with a lower dimension than the original data representation.\n",
    "\n",
    "# FALSE\n",
    "# TRUE  ans\n",
    "\n",
    "#PCA is a dimensionality reduction technique aimed at reducing the number of features (dimensions)\n",
    "#while retaining most of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "363a794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) What is the difference between PCA and Linear Regression?\n",
    "\n",
    "# Vertical distance calculated in PCA, Euclidean distance calculated in Linear Regression\n",
    "# Manhattan distance calculated in PCA, Horizontal distance calculated in Linear Regression\n",
    "# Horizontal distance calculated in PCA, Manhattan distance calculated in Linear Regression.\n",
    "# Euclidean distance calculated in PCA, Vertical distance calculated in Linear Regression. ans\n",
    "\n",
    "#PCA is a dimensionality reduction technique that uses the Euclidean distance to find principal components, \n",
    "# while Linear Regression predicts an outcome using a linear relationship and calculates vertical distances (residuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "029dab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) How to Determine the number of principal components to retain in PCA?\n",
    "\n",
    "# Residual Plot\n",
    "# Elbow plot\n",
    "# None of the above\n",
    "# Cumulative variance plot  ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf26bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) What is the minimum and maximum limit of n_components value we can give in PCA?\n",
    "\n",
    "# Minimum 90% of independent columns count, Maximum 100% of independent columns count\n",
    "# Minimum 1, Maximum 95% of independent columns count\n",
    "# Minimum 2, Maximum 100% of independent columns count\n",
    "# Minimum 95% of independent columns count, Maximum 95% of independent columns count\n",
    "\n",
    "# The minimum value of n_components is 1, meaning at least one principal component must be retained, \n",
    "# and the maximum value can be the total number of independent columns/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ab1ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88f9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Rishi\\Downloads\\GYM\\GYM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc6c4014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_people</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>is_start_of_semester</th>\n",
       "      <th>is_during_semester</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_people  day_of_week  is_weekend  is_holiday  temperature  \\\n",
       "0             37            4           0           0        71.76   \n",
       "1             45            4           0           0        71.76   \n",
       "2             40            4           0           0        71.76   \n",
       "3             44            4           0           0        71.76   \n",
       "4             45            4           0           0        71.76   \n",
       "\n",
       "   is_start_of_semester  is_during_semester  month  hour  \n",
       "0                     0                   0      8    17  \n",
       "1                     0                   0      8    17  \n",
       "2                     0                   0      8    17  \n",
       "3                     0                   0      8    17  \n",
       "4                     0                   0      8    17  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a328596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['hour'])#independent variable\n",
    "y = data['hour']#dependent or target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24315d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X)\n",
    "min_n_components = pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c6b1c",
   "metadata": {},
   "source": [
    "Use the Gym.csv dataset and Apply PCA in the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2924569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19) Minimum n_components required to attain 0.99% of variance: 3\n"
     ]
    }
   ],
   "source": [
    "# 19)  what is the minimum n_components required to attain 0.99% of variance?\n",
    "\n",
    "# 4\n",
    "# 6\n",
    "# 7\n",
    "# 5\n",
    "\n",
    "print(\" Minimum n_components required to attain 0.99% of variance:\", min_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f87471cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20) Difference in sum of variance before and after applying PCA: 15.862366057471945\n"
     ]
    }
   ],
   "source": [
    "# 20) Apply PCA in the given dataset(gym.csv) with n_components as 2, Then get the original (approx.) data back from PCA, \n",
    "#what is the difference in sum of variance of all columns before and after applying PCA?\n",
    "\n",
    "# 83.74\n",
    "# 94.15\n",
    "# 16.26\n",
    "# 5.85\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_original = pca.inverse_transform(X_pca)\n",
    "variance_difference = np.sum(np.var(X, axis=0)) - np.sum(np.var(X_original, axis=0))\n",
    "print(\" Difference in sum of variance before and after applying PCA:\", variance_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ca4211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e0e9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21) Is it appropriate to use PCA as a method for preventing overfitting in machine learning models?\n",
    "# FALSE\n",
    "# TRUE\n",
    "\n",
    "# false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82c02e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of eigenvalues: 21.880026785714282\n"
     ]
    }
   ],
   "source": [
    "# 22) What is the sum of eigenvalues calculated for the first 8 records(gym.csv) with the independent features?\n",
    "# 98.32\n",
    "# 101.76\n",
    "# 102.94\n",
    "# 100.24\n",
    "\n",
    "cov_matrix = np.cov(X.iloc[:8].T)\n",
    "eigenvalues, _ = np.linalg.eig(cov_matrix)\n",
    "sum_eigenvalues = np.sum(eigenvalues)\n",
    "print(\"Sum of eigenvalues:\", sum_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17570533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23) Compare the performance of two Linear Regression models(gym.csv):\n",
    "# The first model uses the raw data without any preprocessing.\n",
    "# The second model applies PCA with n_components set to 4 before fitting the data.\n",
    "# Both models should be generated using train_test_split with test_size=0.2 and random_state=24 during the train-test split. Your task is to calculate the Root Mean Squared Error (RMSE) for both models and determine whether the performance of the second model (PCA-based) is better compared to the first model (raw data).\n",
    "# No change in performance\n",
    "# Model 2 performance is not better than Model 1\n",
    "# Model 2 performance is better than Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "489f95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['hour'], test_size=0.2, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60c4f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model: Linear Regression with raw data\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_test)\n",
    "rmse1 = sqrt(mean_squared_error(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57c8f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model: Linear Regression with PCA\n",
    "pca = PCA(n_components=4)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3d0f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression()\n",
    "model2.fit(X_train_pca, y_train)\n",
    "y_pred2 = model2.predict(X_test_pca)\n",
    "rmse2 = sqrt(mean_squared_error(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "939e4fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23) RMSE for Model 1 (raw data): 5.439104717333018\n",
      "    RMSE for Model 2 (PCA-based): 5.568449048258384\n",
      "    Model 2 performance is not better than Model 1\n"
     ]
    }
   ],
   "source": [
    "print(\"23) RMSE for Model 1 (raw data):\", rmse1)\n",
    "print(\"    RMSE for Model 2 (PCA-based):\", rmse2)\n",
    "if rmse2 < rmse1:\n",
    "    print(\"    Model 2 performance is better than Model 1\")\n",
    "elif rmse2 > rmse1:\n",
    "    print(\"    Model 2 performance is not better than Model 1\")\n",
    "else:\n",
    "    print(\"    No change in performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24) Compute min correlation in the given dataset?\n",
    "# -0.1841\n",
    "# -0.0822\n",
    "# -0.1739\n",
    "# -0.0592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b79432a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum correlation: -0.1739578122762517\n"
     ]
    }
   ],
   "source": [
    "min_correlation = data.corr().min().min()\n",
    "print(\"Minimum correlation:\", min_correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b961358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25) Compare the performance of two Linear Regression models(gym.csv):\n",
    "# The first model uses the raw data without any preprocessing.\n",
    "# The second model applies standardscaler only to independent features and then apply PCA with n_components set to 4 before fitting the data.\n",
    "# Both models should be generated using train_test_split with test_size=0.2 and random_state=24 during the train-test split. Your task is to calculate the Root Mean Squared Error (RMSE) for both models and determine whether the performance of the second model (StandardScalar & PCA-based) is better compared to the first model (raw data).\n",
    "# Model 2 performance is better than Model 1\n",
    "# Model 2 performance is not better than Model 1\n",
    "# No change in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03ac7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4cfe50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the scaled data into train and test sets\n",
    "X_train_scaled, X_test_scaled, _, _ = train_test_split(X_scaled, data['hour'], test_size=0.2, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e830aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model: Linear Regression with raw data\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train_scaled, y_train)\n",
    "y_pred1 = model1.predict(X_test_scaled)\n",
    "rmse1 = sqrt(mean_squared_error(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af30c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model: Linear Regression with PCA\n",
    "pca = PCA(n_components=4)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "486d34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression()\n",
    "model2.fit(X_train_pca, y_train)\n",
    "y_pred2 = model2.predict(X_test_pca)\n",
    "rmse2 = sqrt(mean_squared_error(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae36f67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25) RMSE for Model 1 (raw data): 5.439104717333018\n",
      "    RMSE for Model 2 (StandardScaler & PCA-based): 6.1423227713995185\n",
      "    Model 2 performance is not better than Model 1\n"
     ]
    }
   ],
   "source": [
    "print(\"25) RMSE for Model 1 (raw data):\", rmse1)\n",
    "print(\"    RMSE for Model 2 (StandardScaler & PCA-based):\", rmse2)\n",
    "if rmse2 < rmse1:\n",
    "    print(\"    Model 2 performance is better than Model 1\")\n",
    "elif rmse2 > rmse1:\n",
    "    print(\"    Model 2 performance is not better than Model 1\")\n",
    "else:\n",
    "    print(\"    No change in performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc633b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26) While Selecting Principal Components, the eigenvalues are arranged in descending order, \n",
    "# indicating the amount of variance explained by each component(gym.csv)?\n",
    "# FALSE\n",
    "# TRUE\n",
    "\n",
    "# true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de31a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
